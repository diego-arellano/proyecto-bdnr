* PROYECTO-BDNR

Para este proyecto se utilizó la API de Star Wars gratuita llamada SWAPI, en la cual se almacena la información de personajes, planetas, naves, vehiculos, peliculas, especies de las primeras 7 películas, sí... involucra también a /The Force Awakens/. Nuestra intención fue utilizar esto debido a su facilidad de conexiones entre los personajes con las películas y objetos en los que aparecen o utilizan duranta las películas. 

*** [[https://swapi.py4e.com/documentation][Api utilizada]]

Agregué los comandos necesarios para crear la base de datos en neo4j 

** Mongo 

#+begin_src shell
docker stop mongo
docker rm mongo

export DATA_DIR=`pwd`/data
echo $DATA_DIR
export EX_DIR=`pwd`/mongodb-sample-dataset
echo $EX_DIR
mkdir -p $DATA_DIR
docker run -p 27017:27017 \
       -v $DATA_DIR:/data/db \
       -v $EX_DIR:/mongodb-sample-dataset \
       --name mongo \
       -d mongo
#+end_src


Dentro de Python ejecutar las siguientes sentencias
       - A través de las cuales conectamos mongo con python y creamos las distintas colecciones existentes en mongo, dado que no existe un extract all o algo parecido *nota: la documentación dice que sí se puede a través de https://swapi.py4e.com/api/table/*, pero ¡esto es falso! Solo regresa los primeros 10, por lo tanto tuvimos que extraer todo lo posible iterando
       - Nueva sorpresa: la API no está completa, hay indices de URL que no contienen información y devuelven un error 404 :'( 
       Solución: la iteración se realiza por un while hasta el punto en que se han extraido todos los datos que la api reporta que existen, y el contador no aumenta salvo en el caso de que la respuesta no sea 404. Se apendea en una lista y se utiliza la función insert_many
       
#+begin_src py
import requests
from pymongo import MongoClient

client = MongoClient("mongodb://localhost:27017/")
my_db = client["swapi"]
list=["starships", "vehicles", "species", "films", "planets", "people"]

for table in list:
    collection = my_db[table]
    url = "https://swapi.py4e.com/api/"+table+"/"
    i = 1
    j = 1
    request = requests.get(url)
    response = request.json()
    limite = response["count"]
    print(limite)
    res = []

    while j <= limite:
        url_i = url + str(i)
        req = requests.get(url_i)
        if req.status_code != 404:
            resp = req.json()
            res.append(resp)
            print(j)
            j+=1
        i+=1
    print("fin de tabla"+table)
    collection.insert_many(res)
#+end_src

*** Queries Mongo:
       - Primer query, utilizado para extraer la información de mongo y enviarla a monet: practicamente se desglosan los arreglos existentes dentro de cada uno de los fields del json, pero que considere los casos vacíos y nulos
       #+begin_src js
       db.people.aggregate([{$unwind:{path:"$films",preserveNullAndEmptyArrays: true}},{$unwind:{path:"$species",preserveNullAndEmptyArrays: true}},{$unwind:{path:"$vehicles",preserveNullAndEmptyArrays: true}},{$unwind:{path:"$starships",preserveNullAndEmptyArrays: true}},{$project:{_id:0}},{$out:"people_monet"}])
       #+end_src
       - Segundo query:
       #+begin_src js
              
       #+end_src
       - Tercer query:
       #+begin_src js
              
       #+end_src
          
*Nota: fue importante checar la parte de los delimitadores todavía, porque las comas dentro de los arreglos están ocasionando serios problemas para copiar el csv a monet o a neo
** Monetdb
*** Carga de datos
Los datos se sacarán de MongoDB.

Correr en Mongo para sacar el subset de datos que nos interesan

#+begin_src js
db.people.aggregate([{$unwind:{path:"$films",preserveNullAndEmptyArrays: true}},{$unwind:{path:"$species",preserveNullAndEmptyArrays: true}},{$unwind:{path:"$vehicles",preserveNullAndEmptyArrays: true}},{$unwind:{path:"$starships",preserveNullAndEmptyArrays: true}},{$project:{_id:0}},{$out:"people_monet"}])
#+end_src

Ahora sacamos limpiamos un poco los datos de la base para meterlos a MonetDB
#+begin_src sh
docker exec -i mongo mongoexport --db=swapi --collection=people_monet --type=csv -f name,birth_year,eye_color,gender,hair_color,height,mass,skin_color,homeworld,films,species,starships,vehicles,created,edited,url > people_aux.csv
cat people_aux.csv | sed -e 's/, /-/g' -e 's/none//g' -e 's/n\/a//g' -e 's/unknown//g' >people_monet.csv
#+end_src

*** Inserción de la base people.csv a MonetDB
Inserción de datos en el Docker
#+begin_src sh
docker cp people_monet.csv monetdb:/var/monetdb5/dbfarm/people.csv
#+end_src
Nota:[[https://ugeek.github.io/blog/post/2020-05-27-copiar-directorios-o-archivos-de-un-docker-a-local-o-viceversa.html]][Si se requiere ayuda para el copy a docker]

Creación de la base en MonetDB
#+begin_src sh
monetdb create -p monetdb people
#acceso a monetdb con contraseña monetdb
mclient -u monetdb -d people
#+end_src

*** Creación del esquema People y de la tabla

#+begin_src sql
-- creación de esquema people y usuario people
CREATE USER "people" WITH PASSWORD 'people' NAME 'People Explorer' SCHEMA "sys";
CREATE SCHEMA "people" AUTHORIZATION "people";
ALTER USER "people" SET SCHEMA "people";
-- creacion de la tabla
CREATE TABLE people (
name varchar(35),
birth_year varchar(10),
eye_color varchar(20),
gender varchar(17),
hair_color varchar(20),
height int,
mass float,
skin_color varchar(20),
homeworld varchar(60),
films varchar(60),
species varchar(60),
starships varchar(60),
vehicles varchar(60),
created timestamp,
edited timestamp,
url varchar(60));
-- comando de inserción
copy offset 2 into people from '/var/monetdb5/dbfarm/people.csv' on client using delimiters ',',E'\n',E'\"' null as '';
#+end_src

*** Queries Monetdb
- Primer Query, hacemos una consulta para saber cual es el personaje que aparece en más películas. 
       #+begin_src sql
       select p.name, count(distinct p.films) as cont from people p  group by p.name order by cont desc limit 10;
       #+end_src
- Segundo Query, hacemos una consulta que nos indica las estadisticas
       #+begin_src sql
       select p.skin_color as skin_color, 
       min(distinct p.mass) as min_mass, avg(distinct p.mass) as avg_mass, max(distinct p.mass) as max_mass, sys.var_pop(distinct p.mass) as variance_mass,        
       min(distinct p.height) as min_height, avg(distinct p.height) as avg_height, max(distinct p.height) as max_height,sys.var_pop(distinct p.height) as variance_height,
       count(distinct p.species) as num_species from people p where p.species is not NULL group by skin_color order by num_species desc;
       #+end_src


** Neo4j

Se utilizó una instancia de Neo en AWS de la forma en que se creó en clase.

Para la parte de Neo4j, se exportaron los datos de mongo, completos a csv's para poder importarlo a neo de una forma más sencilla a través de csv's dentro de este mismo github:

#+begin_src sh
declare -A cols=( ["people"]="name,birth_year,eye_color,gender,hair_color,height,mass,skin_color,homeworld,films,species,starships,vehicles,created,edited,url" ["films"]="title,episode_id,opening_crawl,director,producer,release_date,species,starships,vehicles,characters,planets,created,edited,url" ["vehicles"]="name,model,vehicle_class,manufacturer,length,cost_in_credits,crew,passengers,max_atmosphering_speed,cargo_capacity,consumables,films,pilots,created,edited,url" ["starships"]="name,model,starship_class,manufacturer,cost_in_credits,length,crew,passengers,max_atmosphering_speed,hyperdrive_rating,MGLT,cargo_capacity,consumables,films,pilots,created,edited,url" ["planets"]="name,diameter,rotation_period,orbital_period,gravity,population,climate,terrain,surface_water,residents,films,created,edited,url" ["species"]="name,classification,designation,average_height,average_lifespan,eye_colors,hair_colors,skin_colors,language,homeworld,people,films,created,edited,url" )
mkdir swapi_csvs

for col in "${!cols[@]}"
do
    echo $col
    echo ${cols[$col]}
   docker exec -i mongo mongoexport --db=swapi --collection=$col --type=csv --fields=${cols[$col]} > /swapi_csvs/$col.csv
done
#+end_src

Para la importación de los datos dentro de Neo se utilizó el raw_path de cada uno de los csv's durante el load. 


*** Queries Neo:

